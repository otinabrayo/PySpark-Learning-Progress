# PySpark Learning Progress

## Overview
This repository documents my learning journey in PySpark. Below is a list of topics I have covered so far.

## Topics Covered

### Data Reading
- Reading JSON files
- Reading CSV files

### Schema Definition
- Defining schemas using `StructType`


### Data Transformation
- Selecting columns using `select()`
- Renaming columns with `alias()` and `withColumnRenamed()`
- Filtering data using `filter()`
- Adding new columns with `withColumn()`
- Changing data types with `cast()`
- Sorting data using `sort()` / `order()`
- Limiting rows using `limit()`
- Dropping columns with `drop()`
- Removing duplicate rows with `dropDuplicates()`

### Data Operations
- Merging data using `union()` and `unionByName()`
- String functions: `init()`
- Date functions
- Handling null values
- Splitting strings and indexing
- Grouping data using `groupBy()`
- Collecting lists using `collect_list()`
- Pivoting data with `pivot()`
- Conditional operations using `when()` and `otherwise()`
- Performing joins
- Window functions
- User-defined functions (UDFs)

### Data Writing
- Writing data in Parquet format
- Writing data as Tables
- Using Spark SQL

## Next Steps
I plan to explore more advanced PySpark functionalities, including:
- Performance optimization techniques
- Machine learning with PySpark MLlib


## How to Use
This repository will contain example scripts demonstrating these concepts. Stay tuned for updates!

## Contributions
Feel free to fork the repository and contribute by adding examples, optimizations, or new topics.

